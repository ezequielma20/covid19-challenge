{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\emarellano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Libraries for text preprocessing\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('wordnet') \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.sparse import coo_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indico la ruta donde se encuentra el set de datos descargados\n",
    "root_path = 'C:\\\\Users\\\\emarellano\\\\Documents\\\\Ezequiel\\\\Cursos\\\\CORD-19-research-challenge\\\\2020-03-13'\n",
    "\n",
    "#archivo con metadata\n",
    "metadata_path = f'{root_path}/all_sources_metadata_2020-03-13.csv'\n",
    "\n",
    "#leo el archivo con metadata\n",
    "meta_df = pd.read_csv(metadata_path, dtype={\n",
    "    'pubmed_id': str,\n",
    "    'Microsoft Academic Paper ID': str, \n",
    "    'doi': str\n",
    "})\n",
    "#meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13202"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#armo un listado con los archivos json que tienen las noticias\n",
    "all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
    "len(all_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creo un dataframe donde voy a guardar las columnas de interes:\n",
    "#sha: id de la noticia\n",
    "#title: titulo de la noticia\n",
    "#body_text: cuerpo de la noticia\n",
    "df = pd.DataFrame(columns = ['sha', 'title', 'abstract', 'body_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leo cada json y lo agrego al dataframe\n",
    "for json_path in all_json:\n",
    "   with open(json_path) as file:\n",
    "            content = json.load(file)\n",
    "            sha = content['paper_id']\n",
    "            title = content['metadata']['title']\n",
    "            abstract = []\n",
    "            body_text = []\n",
    "            # Abstract\n",
    "            for entry in content['abstract']:\n",
    "                abstract.append(entry['text'])\n",
    "            # Body text\n",
    "            for entry in content['body_text']:\n",
    "                body_text.append(entry['text'])\n",
    "            abstract = '\\n'.join(abstract)\n",
    "            body_text = '\\n'.join(body_text)\n",
    "            row = [(sha, title, abstract, body_text)]\n",
    "            dfRow = pd.DataFrame(row, columns = ['sha', 'title', 'abstract', 'body_text'])\n",
    "            df = pd.concat([dfRow, df], ignore_index=True, sort=False)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ffe133ed880d6c77ae340c5374817232e21f8315</td>\n",
       "      <td>Rational Design of Peptide Vaccines Against Mu...</td>\n",
       "      <td>Human papillomavirus (HPV) occurs in many type...</td>\n",
       "      <td>Certain types of cancers such as liver cancer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ffd3a93b927e221ded4cf76536ad31bef2c74b89</td>\n",
       "      <td>Fatal Respiratory Infections Associated with R...</td>\n",
       "      <td>During an outbreak of severe acute respiratory...</td>\n",
       "      <td>During an outbreak of severe acute respiratory...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ffb381668d93248759ca3855425e05722cb9f562</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>H uman coronaviruses (HCoVs) were first record...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ff7d49ac4008f60ef9c5a437e0d504dcefd1246f</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>results of studies conducted in other countrie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ff365ebbc0fc55476886b0abd129e227c1f8a527</td>\n",
       "      <td>Article focus Hip</td>\n",
       "      <td>We report a systematic review and metaanalysis...</td>\n",
       "      <td>Despite the fact that total hip arthroplasty (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13197</th>\n",
       "      <td>01d162d7fae6aaba8e6e60e563ef4c2fca7b0e18</td>\n",
       "      <td>TWIRLS, an automated topic-wise inference meth...</td>\n",
       "      <td>Faced with the current large-scale public heal...</td>\n",
       "      <td>The sudden outbreak of the new coronavirus (SA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13198</th>\n",
       "      <td>013d9d1cba8a54d5d3718c229b812d7cf91b6c89</td>\n",
       "      <td>Assessing spread risk of Wuhan novel coronavir...</td>\n",
       "      <td>Background: A novel coronavirus (2019-nCoV) em...</td>\n",
       "      <td>In December 2019, a cluster of patients with p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13199</th>\n",
       "      <td>00d16927588fb04d4be0e6b269fc02f0d3c2aa7b</td>\n",
       "      <td>Real-time, MinION-based, amplicon sequencing f...</td>\n",
       "      <td>Infectious bronchitis (IB) causes significant ...</td>\n",
       "      <td>Infectious bronchitis (IB), which is caused by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13200</th>\n",
       "      <td>004f0f8bb66cf446678dc13cf2701feec4f36d76</td>\n",
       "      <td>Healthcare-resource-adjusted vulnerabilities t...</td>\n",
       "      <td></td>\n",
       "      <td>The 2019-nCoV epidemic has spread across China...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13201</th>\n",
       "      <td>0015023cc06b5362d332b3baf348d11567ca2fbb</td>\n",
       "      <td>The RNA pseudoknots in foot-and-mouth disease ...</td>\n",
       "      <td>word count: 194 22 Text word count: 5168 23 24...</td>\n",
       "      <td>VP3, and VP0 (which is further processed to VP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13202 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sha  \\\n",
       "0      ffe133ed880d6c77ae340c5374817232e21f8315   \n",
       "1      ffd3a93b927e221ded4cf76536ad31bef2c74b89   \n",
       "2      ffb381668d93248759ca3855425e05722cb9f562   \n",
       "3      ff7d49ac4008f60ef9c5a437e0d504dcefd1246f   \n",
       "4      ff365ebbc0fc55476886b0abd129e227c1f8a527   \n",
       "...                                         ...   \n",
       "13197  01d162d7fae6aaba8e6e60e563ef4c2fca7b0e18   \n",
       "13198  013d9d1cba8a54d5d3718c229b812d7cf91b6c89   \n",
       "13199  00d16927588fb04d4be0e6b269fc02f0d3c2aa7b   \n",
       "13200  004f0f8bb66cf446678dc13cf2701feec4f36d76   \n",
       "13201  0015023cc06b5362d332b3baf348d11567ca2fbb   \n",
       "\n",
       "                                                   title  \\\n",
       "0      Rational Design of Peptide Vaccines Against Mu...   \n",
       "1      Fatal Respiratory Infections Associated with R...   \n",
       "2                                                          \n",
       "3                                                          \n",
       "4                                      Article focus Hip   \n",
       "...                                                  ...   \n",
       "13197  TWIRLS, an automated topic-wise inference meth...   \n",
       "13198  Assessing spread risk of Wuhan novel coronavir...   \n",
       "13199  Real-time, MinION-based, amplicon sequencing f...   \n",
       "13200  Healthcare-resource-adjusted vulnerabilities t...   \n",
       "13201  The RNA pseudoknots in foot-and-mouth disease ...   \n",
       "\n",
       "                                                abstract  \\\n",
       "0      Human papillomavirus (HPV) occurs in many type...   \n",
       "1      During an outbreak of severe acute respiratory...   \n",
       "2                                                          \n",
       "3                                                          \n",
       "4      We report a systematic review and metaanalysis...   \n",
       "...                                                  ...   \n",
       "13197  Faced with the current large-scale public heal...   \n",
       "13198  Background: A novel coronavirus (2019-nCoV) em...   \n",
       "13199  Infectious bronchitis (IB) causes significant ...   \n",
       "13200                                                      \n",
       "13201  word count: 194 22 Text word count: 5168 23 24...   \n",
       "\n",
       "                                               body_text  \n",
       "0      Certain types of cancers such as liver cancer,...  \n",
       "1      During an outbreak of severe acute respiratory...  \n",
       "2      H uman coronaviruses (HCoVs) were first record...  \n",
       "3      results of studies conducted in other countrie...  \n",
       "4      Despite the fact that total hip arthroplasty (...  \n",
       "...                                                  ...  \n",
       "13197  The sudden outbreak of the new coronavirus (SA...  \n",
       "13198  In December 2019, a cluster of patients with p...  \n",
       "13199  Infectious bronchitis (IB), which is caused by...  \n",
       "13200  The 2019-nCoV epidemic has spread across China...  \n",
       "13201  VP3, and VP0 (which is further processed to VP...  \n",
       "\n",
       "[13202 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#muestro el contenido del dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## analisis de duplicados y nulos\n",
    "# df.loc[0:0,'abstract']\n",
    "# meta_df_not_null = meta_df[meta_df.sha.notnull()] \n",
    "# meta_df['sha']==\"d13a685f861b0f1ba05afa6e005311ad1820fd3a\"\n",
    "# duplicateRowsDF = meta_df_not_null[meta_df_not_null.duplicated(['sha'])]\n",
    "# duplicateRowsDF\n",
    "# result = pd.merge(df,\n",
    "#                 meta_df[['source_x']],\n",
    "#                 on='sha', \n",
    "#                 how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cantidad de palabras en abstract\n",
    "df['abstract_word_count'] = df['abstract'].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cantidad de palabras en body_text\n",
    "df['body_word_count'] = df['body_text'].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    13202.000000\n",
       "mean       210.268596\n",
       "std        196.859557\n",
       "min          1.000000\n",
       "25%        112.000000\n",
       "50%        199.500000\n",
       "75%        270.000000\n",
       "max       4145.000000\n",
       "Name: abstract_word_count, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#estadisticas de abstract\n",
    "df.abstract_word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     13202.000000\n",
       "mean       4236.881836\n",
       "std        4683.207623\n",
       "min           1.000000\n",
       "25%        2458.250000\n",
       "50%        3691.500000\n",
       "75%        5336.750000\n",
       "max      239553.000000\n",
       "Name: body_word_count, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#estadisticas de body_text\n",
    "df.body_word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargar stop_words generico. Se va a utilizar para quitar las palabras \"comunes\" de cada noticia\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agregar stop words propias\n",
    "new_words = [\"many\", \"type\", \"et\", \"al\", \"day\", \"hi\", \"ae\", \"like\", \"common\", \"dc\", \"cd\", \"na\", \"described\", \"medrxiv\", \"preprint\", \"copyright\", \"reviewed\", \"http\", \"doi\", \"author\", \"funder\", \"right\", \"reserved\", \"web\", \"survey\", \"disclosure\", \"permission\", \"granted\", \"license\", \"word\", \"count\", \"biorxiv\", \"display\", \"perpetuity\", \"holder\",  \"reuse\", \"allowed\"]\n",
    "stop_words = stop_words.union(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generar arreglos con la información \"limpia\" de abstract y text body \n",
    "abstract = []\n",
    "body = []\n",
    "for i in range(0, 13202):\n",
    "    #Remove punctuations\n",
    "    text = re.sub('[^a-zA-Z]', ' ', df['abstract'][i])\n",
    "    text2 = re.sub('[^a-zA-Z]', ' ', df['body_text'][i])\n",
    "    #Convert to lowercase\n",
    "    text = text.lower()\n",
    "    text2 = text2.lower()\n",
    "    #remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    text2=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text2)\n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    text2=re.sub(\"(\\\\d|\\\\W)+\",\" \",text2)\n",
    "    ##Convert to list from string\n",
    "    text = text.split()\n",
    "    text2 = text2.split()\n",
    "    ##Stemming\n",
    "    ps=PorterStemmer()\n",
    "    #Lemmatisation\n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    text = [lem.lemmatize(word) for word in text if not word in  \n",
    "            stop_words] \n",
    "    text = \" \".join(text)\n",
    "    abstract.append(text)\n",
    "    \n",
    "    text2 = [lem.lemmatize(word) for word in text2 if not word in  \n",
    "            stop_words] \n",
    "    text2 = \" \".join(text2)\n",
    "    body.append(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se usa CountVectoriser para generar diccionarios de palabras/expresiones con mayor frecuencia. ngram_range indica que van a ser expresiones de 1, 2 y 3 palabras\n",
    "cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
    "\n",
    "X=cv.fit_transform(abstract)\n",
    "X2=cv.fit_transform(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for sorting tf_idf in descending order \n",
    "#https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "df[\"abstract_kw\"] = \"\"\n",
    "df[\"body_kw\"] = \"\"\n",
    "\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X)\n",
    "\n",
    "tfidf_transformer2=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer2.fit(X2)\n",
    "\n",
    "# get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 13202):\n",
    "   # fetch document for which keywords needs to be extracted\n",
    "   doc=abstract[i]\n",
    "   #generate tf-idf for the given document\n",
    "   tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "   #sort the tf-idf vectors by descending order of scores\n",
    "   sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "   #extract only the top n; n here is 5\n",
    "   keywords=extract_topn_from_vector(feature_names,sorted_items,5)\n",
    "   kw = []\n",
    "   for k in keywords:\n",
    "      kw.append(k)\n",
    "   kwstr =  ', '.join(kw) \n",
    "   df.loc[i:i, 'abstract_kw'] = kwstr\n",
    "   \n",
    "   doc2=body[i]\n",
    "   #generate tf-idf for the given document\n",
    "   tf_idf_vector=tfidf_transformer2.transform(cv.transform([doc2]))\n",
    "   #sort the tf-idf vectors by descending order of scores\n",
    "   sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "   #extract only the top n; n here is 5\n",
    "   keywords=extract_topn_from_vector(feature_names,sorted_items,5)\n",
    "   kw = []\n",
    "   for k in keywords:\n",
    "      kw.append(k)\n",
    "   kwstr =  ', '.join(kw) \n",
    "   df.loc[i:i, 'body_kw'] = kwstr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{root_path}/covid_temp_2.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
